{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a43d51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openpyxl\n",
    "#pip install selenium\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc7c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import SyllableTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "import string\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058db636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokens):\n",
    "    \"\"\"\n",
    "    Count tokens in the given text after removing stop words and punctuation marks.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "    - int: The count of tokens after preprocessing.\n",
    "    \"\"\"\n",
    "    # Get the set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove punctuation marks and stop words, and count tokens\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        # Remove punctuation marks\n",
    "        token = token.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Check if the token is not a stop word and not an empty string after removing punctuation\n",
    "        if token.lower() not in stop_words and token != '':\n",
    "            count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c77d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c4e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_record_to_excel(wb, record):\n",
    "    \"\"\"\n",
    "    Write a record to the Excel file.\n",
    "\n",
    "    Args:\n",
    "    - wb (openpyxl.Workbook): The Workbook object.\n",
    "    - record (tuple): The record to write to the file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    ws = wb.active\n",
    "    ws.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab87318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e215215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_excel_file(file_path, headers):\n",
    "    \"\"\"\n",
    "    Create an Excel file with the given file path and headers.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the Excel file.\n",
    "    - headers (list of str): The list of headers.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append(headers)\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a5de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cb73b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_file(url,output_text_file):\n",
    "    \"\"\"\n",
    "    Copy text content of url webpage to text file .\n",
    "\n",
    "    Args:\n",
    "    - url(str) : The url string.\n",
    "    - output_text_file (str): The name of  text file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()  \n",
    "    \n",
    "    # Open the URL in the webdriver\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Extract article heading\n",
    "    heading_element = driver.find_element(By.TAG_NAME, \"h1\")  \n",
    "    article_heading = heading_element.text    \n",
    "    \n",
    "    # Extract article content\n",
    "    content_elements = driver.find_elements(By.CSS_SELECTOR,\"div.td-post-content\");\n",
    "    article_content = '\\n'.join([element.text for element in content_elements])\n",
    "    #print(article_content)\n",
    "    \n",
    "    # Write the article heading and content to the file\n",
    "    with open(output_text_file, \"w\",encoding=\"cp437\", errors='ignore') as file:\n",
    "        file.write(f\"{article_heading}\")\n",
    "        file.write(f\"\\n{article_content}\")\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9582ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc56ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(stopwords_folder_path):\n",
    "    \"\"\"\n",
    "    Read list of stopwatch from files stored in folder \n",
    "    \n",
    "    Args:\n",
    "    - stopwords_folder_path(str) : The url string.\n",
    "    - stopwords_folder_path (str): The folder having files with stopwords.\n",
    "\n",
    "    Returns:\n",
    "    - stop_words_list(list): list of stop words\n",
    "    \"\"\"\n",
    "    \n",
    "    #get list of stopwatch from files stored in folder stopwords_folder_path\n",
    "    \n",
    "    # Initialize an empty list to store words\n",
    "    words_list = []\n",
    "\n",
    "    # Iterate over each file in the folder\n",
    "    for filename in os.listdir(stopwords_folder_path):\n",
    "        # Check if the file is a text file\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(stopwords_folder_path, filename)\n",
    "            # Open the file in read mode\n",
    "            with open(file_path, \"r\") as file:\n",
    "                # Read one line at a time from the file\n",
    "                for line in file:\n",
    "                # Split each line into words\n",
    "                    words = line.split()\n",
    "                    # Extend the words_list with the words from the line\n",
    "                    words_list.extend(words)\n",
    "    stop_words_list=words_list\n",
    "    return(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fe6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb3e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokanize_text(output_text_file):\n",
    "    \"\"\"\n",
    "    Tokanize the text stored in the file. \n",
    "    \n",
    "    Args:\n",
    "    - output_text_file(str) : The source file name.\n",
    "    \n",
    "    Returns:\n",
    "    - tokens(list): list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    #tokanize the content of text file\n",
    "    \n",
    "    # Read the content of the text file\n",
    "    with open(output_text_file, 'r',encoding=\"cp437\", errors='ignore') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "\n",
    "    # Tokenize the text using NLTK's word_tokenize function\n",
    "    tokens = word_tokenize(text)\n",
    "    # Print the list of tokens\n",
    "    #print(tokens)\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c39804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b2b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sylabble_analysis(tokens):\n",
    "    \"\"\"\n",
    "    find and count syllable present in each word excluding word ending with 'es', 'ed'. \n",
    "    \n",
    "    Args:\n",
    "    - tokens(list) : The list of tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - percentage_of_complex_words(list):The total number of complex words / total number of words in a txt file.\n",
    "    - complex_word_count(int) : The number of words having more than 2 syllables \n",
    "    - syllable_per_word(float) :  The number of syllable per word\n",
    "    \"\"\"\n",
    "    \n",
    "    tk = SyllableTokenizer() \n",
    "    complex_word_count = 0\n",
    "    syllable_count=0\n",
    "    percentage_of_complex_words=0\n",
    "    syllable_per_word=0\n",
    "\n",
    "    if len(tokens) > 0 :\n",
    "        for word in tokens:\n",
    "            # Check if the word ends with \"es\" or \"ed\"\n",
    "            if not word.endswith((\"es\", \"ed\")):\n",
    "                syllables = tk.tokenize(word)\n",
    "\n",
    "                #increae syllable count by number of syllables present in the word\n",
    "                syllable_count = syllable_count + len(syllables)\n",
    "\n",
    "                #inccrease comple word count if number of syllables is greater than 2\n",
    "                if len(syllables) > 2:\n",
    "                    complex_word_count += 1\n",
    "\n",
    "        syllable_per_word=syllable_count/len(tokens)\n",
    "        percentage_of_complex_words=complex_word_count/len(tokens)*100\n",
    "    return([percentage_of_complex_words,complex_word_count,syllable_per_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73bde203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronouns(output_text_file):\n",
    "    \"\"\"\n",
    "    find personal pronouns I,we,my,ours as  and count personal pronouns. \n",
    "    \n",
    "    Args:\n",
    "    - output_text_file(str) : The name of text file.\n",
    "    \n",
    "    Returns:\n",
    "    - pronoun_count(int): count of personal pronouns.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_text_file, 'r',encoding=\"cp437\", errors='ignore') as file:\n",
    "        text = file.read()\n",
    "    # Define regex pattern for personal pronouns\n",
    "    pronoun_pattern = r'\\b(?:I|we|my|ours)\\b'\n",
    "\n",
    "    # Compile the regex pattern\n",
    "    pronoun_regex = re.compile(pronoun_pattern, re.IGNORECASE)\n",
    "\n",
    "    # Count occurrences of personal pronouns\n",
    "    pronoun_count = len(re.findall(pronoun_regex, text))\n",
    "\n",
    "    # Exclude occurrences of 'us' when it refers to the country name\n",
    "    country_pattern = r'\\b(?:the\\s+)?US\\b'\n",
    "    country_regex = re.compile(country_pattern, re.IGNORECASE)\n",
    "    country_count = len(re.findall(country_regex, text))\n",
    "\n",
    "    # Subtract the occurrences of 'us' referring to the country name\n",
    "    pronoun_count -= country_count\n",
    "    return(pronoun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f8a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_word_length(tokens):\n",
    "    \"\"\"\n",
    "    find Average Word Length = Sum of the total number of characters in each word/Total number of words. \n",
    "    \n",
    "    Args:\n",
    "    - tokens(list) : The list of tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - average_word_length(float): Average Word Length i.e. total number of characters/total number of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    total_characters = sum(len(token) for token in tokens)\n",
    "\n",
    "    # Calculate the total number of words\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    # Calculate the average word length\n",
    "    if total_words > 0:\n",
    "        average_word_length = total_characters / total_words\n",
    "    else:\n",
    "        average_word_length = 0  # Avoid division by zero if text is empty\n",
    "\n",
    "    return(average_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f46a9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentence_length(output_text_file):\n",
    "    \"\"\"\n",
    "    find Average Sentence Length = the number of words / the number of sentences.\n",
    "    \n",
    "    Args:\n",
    "    - output_text_file(str) : The name of text file.\n",
    "    \n",
    "    Returns:\n",
    "    - average_sentence_length(float): Average Sentence Length i.e. total number of tokens/total number of sentences.\n",
    "    \"\"\"\n",
    "    average_sentence_length=0\n",
    "    # Read the content of the text file    \n",
    "    with open(output_text_file, 'r',encoding=\"cp437\", errors='ignore') as file:\n",
    "        text = file.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    number_of_sentences=len(sentences)\n",
    "    if len(sentences) > 0 :\n",
    "        average_sentence_length = len(tokens)/len(sentences)\n",
    "    return(average_sentence_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75aea74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f44b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_word_per_sentence(output_text_file):\n",
    "    \"\"\"\n",
    "    find Average Number of Words Per Sentence = the total number of tokens / total number of sentences.\n",
    "    \n",
    "    Args:\n",
    "    - output_text_file(str) : The name of text file.\n",
    "    \n",
    "    Returns:\n",
    "    - average_sentence_length(float): Average Number of Words Per Sentence, total number of sentence/total number of tokens\n",
    "    \"\"\"\n",
    "    average_word_per_sentence=0\n",
    "    # Read the content of the text file\n",
    "    with open(output_text_file, 'r',encoding=\"cp437\", errors='ignore') as file:\n",
    "        text = file.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    number_of_sentences=len(sentences)\n",
    "    if len(sentences) > 0 :\n",
    "        average_word_per_sentence = len(tokens)/len(sentences)\n",
    "    \n",
    "    return(average_sentence_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb617f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b918f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(file_path_positive,file_path_negative,filtered_tokens):\n",
    "    \"\"\"\n",
    "    Sentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral.\n",
    "    \n",
    "    Args:\n",
    "    - file_path_positive(str) : The name of text file storing Positive Dictionary with positive words.\n",
    "    - file_path_negative(str) : The name of text file storing Negative Dictionary with negative words.\n",
    "    - filtered_tokens(list) : The list of tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - positive_score(int): +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "    - negative_score(int): -1 for each word if found in the Negative Dictionary and then adding up all the values.\n",
    "    - polarity_score(float): (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001).\n",
    "    - subjectivity_score(float): (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001).\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    # Read the content of the positive token text file\n",
    "    with open(file_path_positive, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Tokenize the text using NLTK's word_tokenize function\n",
    "    poz_tokens = word_tokenize(text)\n",
    "\n",
    "    # Read the content of the negative token text file\n",
    "    with open(file_path_negative, 'r') as file:\n",
    "        text = file.read()\n",
    "    # Tokenize the text using NLTK's word_tokenize function\n",
    "    neg_tokens = word_tokenize(text)\n",
    "    \n",
    "    #intialize positive_score, negative_score to 0\n",
    "    positive_score = 0\n",
    "    negative_score=0\n",
    "    \n",
    "    #count number of positive and negative words in the filtered_text\n",
    "    for token in filtered_tokens:\n",
    "        if token in poz_tokens:\n",
    "            positive_score += 1\n",
    "        if token in neg_tokens:\n",
    "            negative_score -= 1\n",
    "    \n",
    "    #compute ploarity score\n",
    "    polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    #compute subjectivity_score\n",
    "    subjectivity_score = (positive_score + negative_score)/ ((len(filtered_tokens)) + 0.000001)\n",
    "    \n",
    "    #print(positive_score)\n",
    "    #print(negative_score)\n",
    "    #print(polarity_score)\n",
    "    #print(subjectivity_score)\n",
    "    return(positive_score,negative_score,polarity_score,subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632ebab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44c2772a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e278cda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record has been written to the Excel file.\n"
     ]
    }
   ],
   "source": [
    "#from openpyxl import load_workbook\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Specify the URL of the webpage you want to fetch content from\n",
    "    url = \"https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/\"\n",
    "    url_id=\"blackassign0001\"\n",
    "    \n",
    "    #input file name storing url and url_id\n",
    "    input_file_path=\"C:/archana/pythoncode1/20211030 Test Assignment/Input.xlsx\"\n",
    "    \n",
    "    #destination folder name for storing text files\n",
    "    folder_name=\"C:/archana/pythoncode1/20211030 Test Assignment/results\"\n",
    "    \n",
    "    #folder storing stopwatch files\n",
    "    stopwords_folder_path = \"C:/archana/pythoncode1/20211030 Test Assignment/StopWords\"\n",
    "    \n",
    "    # Path to the text file containing positive negative words\n",
    "    file_path_positive = \"C:/archana/pythoncode1/20211030 Test Assignment/MasterDictionary/positive-words.txt\"\n",
    "    file_path_negative = \"C:/archana/pythoncode1/20211030 Test Assignment/MasterDictionary/negative-words.txt\"\n",
    "    \n",
    "    #Path to the excel file to store output metrics\n",
    "    output_file_path = \"C:/archana/pythoncode1/20211030 Test Assignment/results/Output Data Structure.xlsx\"\n",
    "        \n",
    "\n",
    "    # Load the workbook\n",
    "    workbook = load_workbook(input_file_path)\n",
    "\n",
    "    # Select the active worksheet\n",
    "    worksheet = workbook.active\n",
    "    \n",
    "        \n",
    "    # Iterate over rows and read 'url_id' and 'url' from each row\n",
    "    for row in worksheet.iter_rows(min_row=2, values_only=True):  # Assuming the headers are in the first row\n",
    "        url_id, url = row[0], row[1]\n",
    "        \n",
    "        #Initialize all ouput metrics to zero\n",
    "        sentiment_score=[0,0,0,0]\n",
    "        average_sentence_length=0\n",
    "        syllable=[0,0,0]\n",
    "        fog_index=0\n",
    "        average_word_per_sentence=0\n",
    "        word_count=0\n",
    "        personal_pronoun_count=0\n",
    "        average_word_length=0\n",
    "        \n",
    "        #read text from given url and  write text in the file url_id\n",
    "        file_name = f\"{url_id}.txt\"\n",
    "        output_text_file = os.path.join(folder_name, file_name)\n",
    "        \n",
    "        #compute metrics if url exists\n",
    "        response = requests.head(url)\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Save the extracted article text from url to a output_text_file\n",
    "            create_text_file(url,output_text_file)\n",
    "\n",
    "            #get stop words from files stored stopwords_folder_path\n",
    "            stop_words_list=get_stopwords(stopwords_folder_path)\n",
    "\n",
    "            #tokenize text of output_text_file\n",
    "            tokens=tokanize_text(output_text_file)\n",
    "\n",
    "            #delete useless stop_words\n",
    "            filtered_tokens = [token for token in tokens if token not in stop_words_list]  # Filter out tokens to delete\n",
    "\n",
    "            #Sentiment analysis of filtered tokens\n",
    "            sentiment_score=sentiment_analysis(file_path_positive,file_path_negative,filtered_tokens)\n",
    "\n",
    "            #count personal pronouns in the output_text_file\n",
    "            personal_pronoun_count=personal_pronouns(output_text_file)\n",
    "\n",
    "            #count syllable and complex words\n",
    "            syllable=sylabble_analysis(tokens)\n",
    "\n",
    "            #count total cleaned words\n",
    "            word_count=count_words(filtered_tokens)\n",
    "\n",
    "            #calculate average word length\n",
    "            average_word_length=calculate_average_word_length(tokens)\n",
    "\n",
    "            #calculate average sentence length\n",
    "            average_sentence_length=calculate_average_sentence_length(output_text_file)\n",
    "\n",
    "            #calculate average word per sentence\n",
    "            average_word_per_sentence=calculate_average_word_per_sentence(output_text_file)\n",
    "\n",
    "            #calculate Fog Index\n",
    "            fog_index = 0.4 * (average_sentence_length + syllable[0])\n",
    "\n",
    "        #write output metrics to 'Output Data Structure.xlsx file'\n",
    "        headers = ['URL_ID', 'URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH',\n",
    "                  'PERCENTAGE OF COMPLEX WORDS',',FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT',\n",
    "                  'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "\n",
    "        record = (url_id, url,sentiment_score[0],sentiment_score[1],sentiment_score[2],sentiment_score[3],average_sentence_length,\n",
    "                 syllable[0],fog_index,average_word_per_sentence,syllable[1],word_count,syllable[2],personal_pronoun_count,\n",
    "                  average_word_length)\n",
    "\n",
    "        # Check if the Excel file exists. If not create the file with heads\n",
    "        if not os.path.exists(output_file_path):\n",
    "            create_excel_file(output_file_path, headers)\n",
    "\n",
    "        # Load the existing workbook\n",
    "        wb = load_workbook(output_file_path)    \n",
    "\n",
    "        # Write the record to the Excel file\n",
    "        write_record_to_excel(wb, record)\n",
    "\n",
    "        # Save the workbook\n",
    "        wb.save(output_file_path)\n",
    "\n",
    "    print(\"Record has been written to the Excel file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a5fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bca11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
